{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYYzUrBI7CKr6NvaeaBTtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/logprobs/blob/main/Logprobs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logprobs\n",
        "\n",
        "If you are familiar with logits and probability distributions, but curious about log probs, this notebook is for you.\n",
        "\n",
        "The source notebook can be found here: https://github.com/yaoshiang/logprobs/blob/main/Logprobs.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "VsDzEA7qb1ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we need to understand log-probs?\n",
        "\n",
        "Many LLM APIs now expose log-probs of the tokens it generates. This is a high resolution estimate of confidence of the model.\n",
        "\n",
        "Most importantly, **you can use these probabilities** to estimate the probability or confidence of a model when trying to judge its calibration.\n",
        "\n",
        "For example, this is the official Open AI cookbook for working with log probs, and the log-probs allow you to use an LLM as a calibrated classifier:\n",
        "\n",
        "https://cookbook.openai.com/examples/using_logprobs\n"
      ],
      "metadata": {
        "id": "IDDhIAlNuGgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The basics: sigmoid and softmax.\n",
        "\n",
        "Before we get into log-probs, let's cover some basics.\n",
        "\n",
        "Why do we use sigmoid and softmax as activation functions for classifiers?\n",
        "\n",
        "Becuase they turn the output of a neural network model, which are in the range (-inf, inf), into the range (0, 1). And we can interpret numbers in the range (0, 1) as percentages or probabilities.\n",
        "\n",
        "The outputs of a neural network before applying sigmoid or softmax are called logits."
      ],
      "metadata": {
        "id": "9_bdV984uZ6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid\n",
        "\n",
        "Let's make sure we can believe the statement that sigmoid turns logits, or numbers in the range (-inf, inf), into numbers in the range (0, 1). Let's pick some corner case numbers, run sigmoid, and see if the output is indeed in the range (0,1)."
      ],
      "metadata": {
        "id": "CYgfUESgSMvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Make the output human readable.\n",
        "torch.set_printoptions(precision=2, sci_mode=False)"
      ],
      "metadata": {
        "id": "kPmFw0PVSEoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corner_case_logits = torch.Tensor([-5.0, -1.0, 0.0, 1.0, 5])\n",
        "outputs = F.sigmoid(corner_case_logits)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEpTiVLaSev_",
        "outputId": "bd6ccbc0-57d4-44b3-b232-3f0c154b2ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.01, 0.27, 0.50, 0.73, 0.99])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look good! The logits do indeed get squished into the range (0, 1). This means that for the numbers that we get out of a neural network, we can run the sigmoid function and interpret it as a 1% chance, 27% chance, 50% chance, 73% chance, and 99% chance of being a positive."
      ],
      "metadata": {
        "id": "sFOKr15_SrEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax\n",
        "\n",
        "Softmax takes a set of logits and like sigmoid, squishes them into the range (0,1). It has another important attribute: the sum of the numbers will be 1.0, aka 100%. Let's test that empirically.\n",
        "\n",
        "Suppose the output of a model classifying dogs, cats, and birds is 2.0, 1.0, and -1.0. Those clearly don't sum to one. And one of the values is negative! This is definitely not a probability distribution.\n",
        "\n",
        "But after running softmax, we do get a probably distribution: each value is between zero and one, and the sum of 71%, 26%, and 4% is indeed 100%."
      ],
      "metadata": {
        "id": "ucc7cf8ccGQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCP3hMwVbtyJ",
        "outputId": "0e226660-f87d-453a-974f-57aac587995d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a probability distribution and it should sum to one: tensor([0.71, 0.26, 0.04])\n"
          ]
        }
      ],
      "source": [
        "logits = torch.Tensor([2.0, 1.0, -1.0])\n",
        "\n",
        "probability_distribution = F.softmax(logits, dim=-1)\n",
        "print(\"This is a probability distribution and it should sum to one:\", probability_distribution)\n",
        "\n",
        "assert 0.99 < sum(probability_distribution.tolist()) < 1.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax is invariant to scalar shifts.\n",
        "\n",
        "Now let's see what happens if we shift these logits by a constant amount. Let's add 10 to all the logits."
      ],
      "metadata": {
        "id": "DBS1TdFaVY0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_logits = torch.Tensor([12.0, 11.0, 9.0])\n",
        "probability_distribution = F.softmax(shifted_logits, dim=-1)\n",
        "print(\"This is a probability distribution and it should sum to one:\", probability_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc1ILi2FVf9p",
        "outputId": "d137b1c5-d5d7-4016-bac6-7d5441ef65a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a probability distribution and it should sum to one: tensor([0.71, 0.26, 0.04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! The output is the same! The fancy way of saying this is that the softmax function is invariant to constant shifts in the input logits."
      ],
      "metadata": {
        "id": "38JkoL9NVuDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Instability\n",
        "\n",
        "However, just because softmax is mathematically invariant to constant shifts, doesn't mean we can go crazy when actually doing math on silicon. A 32 bit floating point number has about 7 significant digits (ask ChatGPT about mantissa and exponent if you wanna learn more).\n",
        "\n",
        "So if we add big number, like 10,000,000, the computer will round away the 2, 1 and -1 and just see 10,000,000. And running softmax of a vector of three numbers, all of which are the same, will result in a uniform distribution: 33.3%, 33.3%, 33.3%."
      ],
      "metadata": {
        "id": "R13lktwTWhGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_logits = logits + 1000000000000000\n",
        "probability_distribution = F.softmax(shifted_logits, dim=-1)\n",
        "print(\"This is not the probability distribution we wanted.\", probability_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrab8c6qWSY3",
        "outputId": "f9881565-2153-477d-f2b5-37099c786359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is not the probability distribution we wanted. tensor([0.33, 0.33, 0.33])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem actually gets worse than that. When training a neural network, we run an algorithm called back propagation, which requires that we calculate gradients of a function. Gradients are the first-order derivative - the slope of a line. If the line is really flat, your gradients will round down to zero and your model won't train. If the line is really vertical, your gradients will approach infinity.\n",
        "\n",
        "Unfortunately, the softmax function uses exponents, and the loss function of a model uses logs. Both of these functions can get very vertical and very flat at the extremes. This means that for the shifted logits, even if they should give the same numbers after running softmax, due to numerical instability, you'll instead get a NaN or inf.\n",
        "\n",
        "Let's run a simple backprop algorithm in pytorch to prove this.\n",
        "\n",
        "Notice that the gradients from the big logits (102 and 101) are NaN, or Not A Number. This means the values overflowed, which would ruin any neural network training."
      ],
      "metadata": {
        "id": "IjFU2JpyXw2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_backprop(logits):\n",
        "  # Run softmax.\n",
        "  exp_logits = torch.exp(logits)\n",
        "  sum_exp_logits = torch.sum(exp_logits)\n",
        "  prediction = exp_logits / sum_exp_logits\n",
        "\n",
        "  loss = torch.log(prediction).mean()\n",
        "  loss.backward()\n",
        "  return prediction, logits.grad\n",
        "\n",
        "# Create a small input value\n",
        "input = torch.tensor([2.0, 1.0], requires_grad=True)\n",
        "shifted_input = torch.tensor([102.0, 101.0], requires_grad=True)\n",
        "\n",
        "# Print the gradients\n",
        "print(f\"Gradient for: {input.tolist()}: \", simple_backprop(input)[1])\n",
        "print(f\"Gradient for: {shifted_input.tolist()}: \", *simple_backprop(shifted_input)[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtq30q62rUx",
        "outputId": "c3aeee7c-34ca-4997-9353-19aceec327a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient for: [2.0, 1.0]:  tensor([-0.23,  0.23])\n",
            "Gradient for: [102.0, 101.0]:  tensor(nan) tensor(nan)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the gradients from the big logits (102 and 101) are NaN, or Not A Number. This means the values overflowed, which would ruin any neural network training."
      ],
      "metadata": {
        "id": "2joWECoO6JKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diving into the math to find the numerically stable log-softmax function.\n",
        "\n",
        "We are going to have to dig into the greek letters to figure out how to stabilize this math.\n",
        "\n",
        "Softmax is defined as:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
        "\n",
        "Where $z_i$ is the logit for class $i$.\n",
        "\n",
        "Notice all those exponents... running $e^{101}$ is what caused our previous code to overflow into NaNs.\n",
        "\n",
        "Crossentropy is the loss function for a probability distribution, based on the basic theory of [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) which underpins neural network training. Crossentropy is defined as:\n",
        "\n",
        "$$CE = -\\sum_{i=1}^K y_i \\log(p_i)$$\n",
        "\n",
        "Where $y_i$ is really just a switch that turns on a specific probability $p_i$ for the true label (e.g. dog), and $p_i$ is the output of the softmax function.\n",
        "\n",
        "Combining the two equations allows us to calculate crossentropy on a logit:\n",
        "\n",
        "$$CE = -\\sum_{i=1}^K y_i \\log\\left(\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\right)$$\n",
        "\n",
        "We know that the $\\log(\\frac{a}{b}) = \\log(a) - \\log(b)$, and $\\log(e^x) = x$ so we can adjust the above to\n",
        "\n",
        "$$ = -\\sum_{i=1}^K y_i \\left(z_i - \\log\\left(\\sum_{j=1}^K e^{z_j}\\right)\\right)$$\n",
        "\n",
        "Let's look deeper inside the key of that equation: it appears to be just a constant shift of all the logits, $z_1, z_2, z_3, ..., z_k$. In fact, that equation is the log-softmax of the logits.\n",
        "\n",
        "$$ \\text{logsoftmax}(z_i) = z_i - \\log\\sum_{j=1}^K e^{z_j}$$\n",
        "\n",
        "We have freed the logits $z_1, z_2, z_3, ..., z_k$ from that nasty log-exp combo, but, we haven't solved the calculation of the right hand side of the equation. We still see the log of the sum of the exponent of a series of numbers. Are we still stuck taking the exponent of big numbers?\n",
        "\n",
        "Luckily, we have the log-sum-exp trick to stabilize that part of the equation.\n"
      ],
      "metadata": {
        "id": "8LfVIXVDK1Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-Sum-Exp\n",
        "\n",
        "Anytime you are taking the log of the sum of the exponents, know that you can factor out some of the big numbers (e.g $e^{101}$) using the log-sum-exp technique.\n",
        "\n",
        "First, we can immediately subtract a big number $M$ from all the logits, then multiply the resulting value by $e^M$.\n",
        "\n",
        "$$\\log \\left( \\sum_{i=1}^n e^{z_i} \\right) = \\log \\left( e^M \\sum_{i=1}^n e^{z_i - M} \\right)$$\n",
        "\n",
        "Then, we find the log of a product of two values, and can move that to be the sum of two logs.\n",
        "\n",
        "$$ = \\log(e^M) + \\log \\left( \\sum_{i=1}^n e^{x_i - M} \\right)$$\n",
        "\n",
        "And we know the log of an exponent of a value is just the value...\n",
        "\n",
        "$$ = M + \\log \\left( \\sum_{i=1}^n e^{x_i - M} \\right)$$\n",
        "\n",
        "so we have effectively reduced the scale of our logits while adding just a constant value to the log-sum-exp of the reduced logits. Voila! Numerical stability.\n",
        "\n",
        "The log-sum-exp trick uses the maximum M, but we need a specific M that generates the exact log-probs - but since our numbers are numerically stable, we can pass the log-sum-exp through a second time and be confident it will be a numerically stable calculation to essentilaly get a second $M$ value to add to the logits."
      ],
      "metadata": {
        "id": "NVFHNUhLmAcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nice Properties of Log-Probs\n",
        "\n",
        "The log prob output by APIs like Open AI are really just the logits shifted by a constant amount so that they have some nicer properties than raw logits.\n",
        "\n",
        "Since probabilities are always between zero and one, the log-prob will always be less than zero. A high confidence prediction would have a prob of say 0.999, yielding a log-prob of something like -1e-3. A low confidence prediction would have a log prob that looks something like -0.5.\n",
        "\n",
        "If you want to calculate the actual probability of a specific token, just take the exponent of the log-prob. No need for worrying about the other [30,521] possible tokens that the model output to calculate a sum - a single log-prob for a single token is all you need to know the probability of that token.\n",
        "\n",
        "Log probs always can be flowed into a loss function almost directly, without need for logs or exponents or crossentropy or softmax. The loss function for this in pytorch is\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
        "\n"
      ],
      "metadata": {
        "id": "Wy9Sm9KQbvRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing log probs"
      ],
      "metadata": {
        "id": "nqzSWOtifYt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to prove some of the math above. Let's set up some logits, calculate the log-softmax. We see that indeed, all the values are negative."
      ],
      "metadata": {
        "id": "oRJzaw1Hb37H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([102.0, 101.0])\n",
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "print(log_probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stOoZ1nlb4x2",
        "outputId": "29ccf3cf-ef58-4347-ad76-dbe2221ee61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.31, -1.31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see if the logits and the log-probs output the same probability distribution. Yup, that checks out too!"
      ],
      "metadata": {
        "id": "tHEiZ4kMduxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(F.softmax(logits, dim=-1))\n",
        "print(F.softmax(log_probs, dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VxCbrpAd54M",
        "outputId": "b62ae6ad-2025-4432-a99c-679ea239fe66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.73, 0.27])\n",
            "tensor([0.73, 0.27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's prove that we can calculate the probabilities simply by running the exp function, not the softmax, when we have the log_probs. Once again, things look good.\n"
      ],
      "metadata": {
        "id": "i7UaBrrKeOZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.exp(log_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-_s10deNH7",
        "outputId": "7d9b29cf-d1e5-403f-8fc6-dc236238bf5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.73, 0.27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's make sure that this is indeed numerically stable. Can we get good gradients with big numbers? Recall that we previously ran a naive approach to calculcating this gradient and got NaN for gradients."
      ],
      "metadata": {
        "id": "zDzj93IEef0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_input = torch.tensor([102.0, 101.0], requires_grad=True)\n",
        "log_probs = F.log_softmax(shifted_input, dim=-1)\n",
        "log_probs.retain_grad()\n",
        "print(f\"Gradients for: {shifted_input.tolist()}: \", simple_backprop(log_probs)[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEJHBhA8eeQA",
        "outputId": "a8a8d1b9-b86a-4490-b77e-0761b2a3e786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for: [102.0, 101.0]:  tensor([-0.23,  0.23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yup! We got good gradients despite large logits!"
      ],
      "metadata": {
        "id": "4Ufw9b0jhJ3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAP Generative AI Hub\n",
        "\n",
        "If you're an SAP customer, you may be accessing Open AI models through [SAP Generative AI Hub](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/consume-generative-ai-models-using-sap-ai-core).\n",
        "\n",
        "According to the documentation there, when you access Open AI models there, you are actually accessing Azure Open AI models and indeed, the SAP documentation points you to the [Azure Open AI docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions).\n",
        "\n",
        "And Azure Open AI, like the direct Open AI API, gives you the ability to access the log-probs outputted by an LLM like GPT-4o. With those logprobs, you can build calibrated classifiers and maybe even double check if your chatbot answers aren't as good as they could be."
      ],
      "metadata": {
        "id": "gWzBm-RU8X7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced loss functions in TensorFlow\n",
        "\n",
        "If you've been programming in Tensorflow, you might be a little worried right now - you've been activating your neural networks with Softmax and then using the categorical_crossentropy loss function, just like you were taught in that basic ML course. Were you risking numerical instability?\n",
        "\n",
        "TF/Keras ignores what you implemented and operates directly on the logits. See this code block:\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/e7e2b655d573de99c7c7fd2ceb5659f57fb2d908/tensorflow/python/keras/backend.py#L4762\n",
        "\n",
        "```python\n",
        "  # Use logits whenever they are available. `softmax` and `sigmoid`\n",
        "  # activations cache logits on the `output` Tensor.\n",
        "  if hasattr(output, '_keras_logits'):\n",
        "    output = output._keras_logits  # pylint: disable=protected-access\n",
        "    if from_logits:\n",
        "      warnings.warn(\n",
        "          '\"`categorical_crossentropy` received `from_logits=True`, but '\n",
        "          'the `output` argument was produced by a sigmoid or softmax '\n",
        "          'activation and thus does not represent logits. Was this intended?\"')\n",
        "    from_logits = True\n",
        "```\n",
        "\n",
        "Then, when it's operating on the logits, it uses the log_softmax function to apply the constant shift of the logits by the numerically stable implementation of log-sum-exp.\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/e7e2b655d573de99c7c7fd2ceb5659f57fb2d908/tensorflow/python/ops/nn_ops.py#L4149C8-L4149C49\n",
        "\n",
        "```python\n",
        "    # Do the actual op computation.\n",
        "    if config.is_op_determinism_enabled():\n",
        "      log_probs = log_softmax_v2(precise_logits)\n",
        "      cost = -math_ops.reduce_sum(labels * log_probs, axis=1)\n",
        "```"
      ],
      "metadata": {
        "id": "wt0_x1MBbnn8"
      }
    }
  ]
}